{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ingest Web of Science csv files into mySQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "import pandas as pd\n",
    "#import mysql.connector as mysql\n",
    "import pymysql\n",
    "import time\n",
    "import os\n",
    "\n",
    "master_timer_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#      specifies how to recognize the partial file splits:\n",
    "\n",
    "data_dir = '/project2/jevans/study_dbs/data/full_v1/'\n",
    "\n",
    "pubs_substring = 'wos_cut_publications'  # postfixed with e.g. batch_0.csv\n",
    "contrib_substring = 'wos_cut_contributors'\n",
    "inst_substring = 'wos_cut_institutions'\n",
    "ref_path = 'wos_cut_refs.csv'\n",
    "wosIDs_path = 'wos_cut_wosids.csv'\n",
    "\n",
    "# sql alchemy connection string:\n",
    "db_name = 'test_wos_cut_full'\n",
    "connect_string = \"mysql+pymysql:///{}?unix_socket=/project2/jevans/study_dbs/mysql/.sql.sock\".format(db_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the database\n",
    "\n",
    "client_config = {'unix_socket':'/project2/jevans/study_dbs/mysql/.sql.sock'}\n",
    "db = pymysql.connect(**client_config)\n",
    "\n",
    "\n",
    "\n",
    "cursor = db.cursor()\n",
    "sql = \"CREATE DATABASE {}\".format(db_name)\n",
    "try:\n",
    "    cursor.execute(sql)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "cursor.close()\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a dictionary for to keep track of file names, etc:\n",
    "\n",
    "tables = {}\n",
    "tables['publications'] = {'substring_filemarker':pubs_substring}\n",
    "tables['contributors'] = {'substring_filemarker':contrib_substring}\n",
    "tables['institutions'] = {'substring_filemarker':inst_substring}\n",
    "tables['the_references'] = {'substring_filemarker':ref_path}\n",
    "tables['wosIDs'] = {'substring_filemarker':wosIDs_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort out the matching file splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing publications ...\n",
      "publications ingested in 575.8294894695282 s\n",
      "processing contributors ...\n",
      "contributors ingested in 465.612087726593 s\n",
      "processing institutions ...\n",
      "institutions ingested in 18.747880697250366 s\n",
      "processing the_references ...\n",
      "the_references ingested in 1522.6044552326202 s\n",
      "processing wosIDs ...\n",
      "wosIDs ingested in 87.0508086681366 s\n"
     ]
    }
   ],
   "source": [
    "all_datafiles = os.listdir(data_dir)\n",
    "\n",
    "for k, v in iter(tables.items()):\n",
    "    \n",
    "    print('processing {} ...'.format(k))\n",
    "    start_time = time.time()\n",
    "    \n",
    "    tables[k]['files'] = [f for f in all_datafiles \\\n",
    "                          if (v['substring_filemarker'] in f)]\n",
    "    DFs = []\n",
    "    for filename in tables[k]['files']:\n",
    "        path = os.path.join(data_dir,filename)\n",
    "        #print(path)\n",
    "        DFs.append(pd.read_csv(path, sep=','))\n",
    "\n",
    "    # join dfs\n",
    "    df = pd.concat(DFs, axis=0, ignore_index=True)        \n",
    "\n",
    "    # insert into db\n",
    "    sql_engine = sqlalchemy.create_engine(connect_string)\n",
    "    df.to_sql(con=sql_engine,\n",
    "             name=k,\n",
    "             if_exists='replace',\n",
    "             chunksize=50000)\n",
    "        \n",
    "    end_time = time.time()\n",
    "    print('{} ingested in {} s'.format(k, end_time-start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total elapsed time during ingestion: 2669.903683900833 s\n"
     ]
    }
   ],
   "source": [
    "master_timer_stop = time.time()\n",
    "print('total elapsed time during ingestion: {} s'.format(master_timer_stop - master_timer_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
